{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -r /kaggle/input/test-task-kvant-data/requirements-3.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers==4.49.0\n",
    "!pip uninstall -y numpy scipy\n",
    "!pip install --upgrade --force-reinstall numpy==1.24.0 scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T11:20:31.095478Z",
     "iopub.status.busy": "2025-07-06T11:20:31.095252Z",
     "iopub.status.idle": "2025-07-06T11:20:58.002831Z",
     "shell.execute_reply": "2025-07-06T11:20:58.002050Z",
     "shell.execute_reply.started": "2025-07-06T11:20:31.095454Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-06 11:20:47.560804: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1751800847.756351      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1751800847.811143      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from uuid import uuid4\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain_community.docstore.document import Document\n",
    "import os, json\n",
    "import re\n",
    "import difflib\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T11:20:58.004880Z",
     "iopub.status.busy": "2025-07-06T11:20:58.004356Z",
     "iopub.status.idle": "2025-07-06T11:20:59.861924Z",
     "shell.execute_reply": "2025-07-06T11:20:59.860914Z",
     "shell.execute_reply.started": "2025-07-06T11:20:58.004860Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01e49d60d9d249898e32e679ed3157df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/719 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfd0440b5a054f6397b5997a7895e4b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "part.0.parquet:   0%|          | 0.00/797k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09f28ec5cbfe46f6b7bbee700a7b2bf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating passages split:   0%|          | 0/3200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"rag-datasets/rag-mini-wikipedia\", \"text-corpus\")[\"passages\"]\n",
    "dataset_small = dataset.select(range(2000))\n",
    "\n",
    "documents = []\n",
    "for example in dataset_small:\n",
    "    passage = example['passage']\n",
    "    doc_id = str(uuid4())\n",
    "    documents.append({'id': doc_id, 'text': passage})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name='/kaggle/input/test-task-kvant-data/all-MiniLM-L6-v2/all-MiniLM-L6-v2')\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"mini_wiki_collection\",\n",
    "    embedding_function=embedding_model,\n",
    "    persist_directory=\"./chroma_db\",\n",
    ")\n",
    "\n",
    "chunks = [Document(page_content=d['text'], metadata={\"id\": d['id']}) for d in documents]\n",
    "ids = [d['id'] for d in documents]\n",
    "vector_store.add_documents(documents=chunks, ids=ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T11:21:11.183532Z",
     "iopub.status.busy": "2025-07-06T11:21:11.182841Z",
     "iopub.status.idle": "2025-07-06T11:21:46.087800Z",
     "shell.execute_reply": "2025-07-06T11:21:46.086946Z",
     "shell.execute_reply.started": "2025-07-06T11:21:11.183512Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8507f3a035c4b11b399909a6743e936",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "/tmp/ipykernel_35/1309108278.py:11: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=generator_pipeline)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/test-task-kvant-data/Llama-3.2-3B-Instruct/Llama-3.2-3B-Instruct')\n",
    "model = AutoModelForCausalLM.from_pretrained('/kaggle/input/test-task-kvant-data/Llama-3.2-3B-Instruct/Llama-3.2-3B-Instruct')\n",
    "\n",
    "generator_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=100,\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=generator_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install evaluate rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T11:21:52.058071Z",
     "iopub.status.busy": "2025-07-06T11:21:52.057786Z",
     "iopub.status.idle": "2025-07-06T11:21:52.083853Z",
     "shell.execute_reply": "2025-07-06T11:21:52.083297Z",
     "shell.execute_reply.started": "2025-07-06T11:21:52.058045Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DB_PATH = \"chat_history.db\"\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "c = conn.cursor()\n",
    "\n",
    "c.execute('''\n",
    "CREATE TABLE IF NOT EXISTS chat_history (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    question TEXT NOT NULL,\n",
    "    answer TEXT NOT NULL\n",
    ")\n",
    "''')\n",
    "conn.commit()\n",
    "\n",
    "def save_history(question, answer):\n",
    "    conn = sqlite3.connect('chat_history.db')\n",
    "    c = conn.cursor()\n",
    "    c.execute(\"INSERT INTO chat_history (question, answer) VALUES (?, ?)\", (question, answer))\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def extract_final_answer(full_output):\n",
    "    import re\n",
    "    match = re.search(r'Provide your final answer below:\\s*(.*)', full_output, re.DOTALL | re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        return full_output.strip()\n",
    "\n",
    "def parse_answer_from_output(output):\n",
    "    pattern = r\"Provide your final answer below.*?\\n([\\s\\S]+)\"\n",
    "    match = re.search(pattern, output, re.IGNORECASE)\n",
    "    \n",
    "    if match:\n",
    "        answer_part = match.group(1).strip()\n",
    "        for line in answer_part.splitlines():\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                line = re.sub(r\"ðŸ“ Answer:.*\", \"\", line).strip()\n",
    "                line = line.replace('\"', '').replace(\"'\", \"\").strip()\n",
    "                if line.endswith('.'):\n",
    "                    line = line[:-1].strip()\n",
    "                line = line.lower()\n",
    "                return line\n",
    "        return answer_part\n",
    "    else:\n",
    "        output = output.strip()\n",
    "        output = output.replace('\"', '').replace(\"'\", \"\").strip()\n",
    "        if output.endswith('.'):\n",
    "            output = output[:-1].strip()\n",
    "        output = output.lower()  \n",
    "        return output \n",
    "        \n",
    "def rag_chat(question, k=3):\n",
    "    retriever = vector_store.as_retriever(search_kwargs={'k': k})\n",
    "    docs = retriever.invoke(question)\n",
    "    knowledge = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are a highly knowledgeable and concise AI assistant.\n",
    "\n",
    "    Your role is to help the user by answering their questions **clearly, accurately, and extremely concisely**, using ONLY the information provided to you in the context section below.\n",
    "    \n",
    "    Guidelines:\n",
    "    - Provide a **direct and minimal answer** to the user's question, ideally **one word, a number, or a very short phrase**.\n",
    "    - Do **NOT** provide long explanations or additional information.\n",
    "    - Do **NOT** mention or refer to any context, documents, or sources you used for your answer.\n",
    "    - Do **NOT** use any external or prior knowledge that is not included in the provided context.\n",
    "    - Your tone should be **neutral, confident, and professional**.\n",
    "    \n",
    "    Here is the userâ€™s question:\n",
    "    \n",
    "    {question}\n",
    "    \n",
    "    Here is the context you must use to answer:\n",
    "    \n",
    "    {knowledge}\n",
    "    \n",
    "    Remember: If the context does not contain sufficient information to answer the question, say **\"I don't know.\"**\n",
    "    \n",
    "    Provide your final answer below (ONLY the answer, nothing else):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # print(f'KNOWLEGE >>>>>>>>>>> {knowledge}')\n",
    "    \n",
    "    raw_response = llm(prompt)\n",
    "    \n",
    "\n",
    "    if isinstance(raw_response, list) and len(raw_response) > 0 and 'generated_text' in raw_response[0]:\n",
    "        raw_text = raw_response[0]['generated_text']\n",
    "    elif isinstance(raw_response, str):\n",
    "        raw_text = raw_response\n",
    "    else:\n",
    "        raw_text = str(raw_response)\n",
    "\n",
    "    response = parse_answer_from_output(raw_text)\n",
    "\n",
    "    print(\"MODEL ANSWER >>>>>>>>>>\", response)\n",
    "\n",
    "    save_history(question, response)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T11:21:52.085183Z",
     "iopub.status.busy": "2025-07-06T11:21:52.084922Z",
     "iopub.status.idle": "2025-07-06T11:21:52.091181Z",
     "shell.execute_reply": "2025-07-06T11:21:52.090341Z",
     "shell.execute_reply.started": "2025-07-06T11:21:52.085162Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# query = \"What is Uruguay?\"\n",
    "# answer = rag_chat(query)\n",
    "# print(\"ðŸ“ Answer:\", answer)\n",
    "\n",
    "# c.execute(\"SELECT * FROM chat_history\")\n",
    "# rows = c.fetchall()\n",
    "# for row in rows:\n",
    "#     print(row)\n",
    "\n",
    "# conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T11:26:10.257953Z",
     "iopub.status.busy": "2025-07-06T11:26:10.257630Z",
     "iopub.status.idle": "2025-07-06T11:30:39.726401Z",
     "shell.execute_reply": "2025-07-06T11:30:39.725608Z",
     "shell.execute_reply.started": "2025-07-06T11:26:10.257931Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "import evaluate\n",
    "dataset_qa = load_dataset(\"rag-datasets/rag-mini-wikipedia\", \"question-answer\")\n",
    "dataset_eval = dataset_qa[\"test\"].select(range(100))  \n",
    "\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def exact_match(prediction, references):\n",
    "    pred_norm = prediction.strip().lower()\n",
    "    for ref in references:\n",
    "        if pred_norm == ref.strip().lower():\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def fuzzy_exact_match(prediction, references, threshold=0.8):\n",
    "    pred_norm = prediction.strip().lower()\n",
    "    for ref in references:\n",
    "        ref_norm = ref.strip().lower()\n",
    "        ratio = difflib.SequenceMatcher(None, pred_norm, ref_norm).ratio()\n",
    "        if ratio >= threshold:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def normalize_case(text):\n",
    "    if isinstance(text, list):\n",
    "        return [t.lower().strip() for t in text]\n",
    "    elif isinstance(text, str):\n",
    "        return text.lower().strip()\n",
    "    else:\n",
    "        return text\n",
    "        \n",
    "def semantic_similarity(prediction, references):\n",
    "    pred_emb = embedding_model.embed_query(prediction)\n",
    "    ref_embs = [embedding_model.embed_query(ref) for ref in references]\n",
    "    sims = cosine_similarity([pred_emb], ref_embs)\n",
    "    return np.max(sims)\n",
    "\n",
    "exact_matches = []\n",
    "semantic_similarities = []\n",
    "bleu_scores = []\n",
    "rouge_scores = []\n",
    "\n",
    "for example in dataset_eval:\n",
    "    question = example[\"question\"]\n",
    "    references = normalize_case(example[\"answer\"])\n",
    "    print(f'EXAMPLE ANSWER >>>>>>>>>>>>> {references}')\n",
    "    \n",
    "    prediction = rag_chat(question)  \n",
    "    \n",
    "    em = fuzzy_exact_match(prediction, references, threshold=0.8)\n",
    "    exact_matches.append(em)\n",
    "    \n",
    "    ss = semantic_similarity(prediction, references)\n",
    "    semantic_similarities.append(ss)\n",
    "    \n",
    "    rouge = rouge_metric.compute(\n",
    "        predictions=[prediction],\n",
    "        references=[references]  \n",
    "    )\n",
    "    rouge_scores.append(rouge[\"rougeL\"])  \n",
    "\n",
    "print(f\"Fuzzy Exact Match (EM) score: {np.mean(exact_matches):.3f}\")\n",
    "print(f\"Semantic Similarity score: {np.mean(semantic_similarities):.3f}\")\n",
    "print(f\"ROUGE-L score: {np.mean(rouge_scores):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7800789,
     "sourceId": 12380743,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
